# Discussion {#disc}

<!--
Our results so far suggest that some insights can be taken by predicting basketball shooting outcomes, but none of our results are more insightful than common intuition (i.e. the most frequent shooters are usually above average shooters, or the probability of making a shot decreases as distance from the basket increases). None of the models that we have built predict the out-of-sample data well. We could refine our models by including more predictors such as a proxy for fatigue (using information about total minutes played, or consecutive minutes played without a timeout), or shot difficulty (using information about the nearest defender). -->

## Evaluation of Models

To evaluate these models, we use 5-fold cross-validation. This process used as many as 20 simultaneous RStudio Pro servers provided by the Duke University Statistical Science Department. In each train-test split, we evaluate a model's out-of-sample classification rate (using a cutoff probability of 0.5), Brier score (mean squared error), and log likelihood. The predictions and fitted values are obtained using MCMC averages; to calculate the probability for an individual shot, we calculate a response for each of the 9,500 posterior simulations, then take the average of those responses. The results are plotted below:

```{r predfxns, cache=TRUE}
# posterior mean   --> minimize sq err
# posterior median --> minimize abs err

# we want S predictions per shot. an N x S matrix. then we take MCMC average.
pred_glm <- function(Xpred = NA, MCMC = NA){
  
  Xmat <- Xpred %>% 
    mutate(logr = log(r),
           away = 1-home) %>%
    arrange(time) %>%
    select(away, home, logr, theta) %>% 
    as.matrix()

  # the baseline 1 intercept = "away"
  # we should only multiply by that one if home = 0
  #params.glm <- t(MCMC)
  A <- Xmat
  B <- t(MCMC)
  pred.glm <-  arm::invlogit(A %*% B) %>% apply(., 1, median) #MCMC median
  
  return(as.matrix(pred.glm))
}

pred_me <- function(Xpred = NA, MCMC = NA){

  Xmat <- Xpred %>% 
    mutate(logr = log(r),
           away = 1-home) %>%
    arrange(time) %>%
    select(away, home, logr, theta, globalplayerid) %>% 
    as.matrix()

  pred.me <- array(NA, c(nrow(Xmat), 1))
  for(r in 1:nrow(Xmat)){
    gpid <- Xmat[r,"globalplayerid"]
    Asub <- Xmat[r,1:4] # 1 x p
    playerinds <- grep(gpid, colnames(MCMC))
    Bsub <- t(MCMC[,playerinds]) # p x S
    if(length(playerinds) == 0){
      Bsub <- t(MCMC[,which(!grepl("\\]", colnames(MCMC)))])
    }
    pred.me[r,] <- median(arm::invlogit(Asub %*% Bsub)) #MCMC avg
  }

  return(pred.me)
}

pred_disc <- function(Xpred = NA, MCMC.LIST=NA){
  
  pred.disc.large <- lapply(X=MCMC.LIST, FUN=pred_me, Xpred=Xpred) %>% 
    as.data.frame() %>%
    as.matrix()
  
  rownames(pred.disc.large) <- Xpred$gameid   #N
  colnames(pred.disc.large) <- gamemap$gameid #G, from the list
  
  pred.disc <- array(NA, c(nrow(Xpred), 1))

  for(i in 1:nrow(Xpred)){
    keepcol <- colnames(pred.disc.large) == Xpred[i,"gameid"]
    pred.disc[i,] <- pred.disc.large[i, keepcol]
  }

  return(pred.disc)
  
}


if(!load_chains){
  
  pred.team     <- pred_glm(Xtest, glmtot)
  pred.player   <- pred_me(Xtest, player.mcmc)
  eval(parse(
    text=paste0("pred.game.",deltas_str," <- pred_disc(Xtest, game.mcmc.list.", deltas_str, ");\nprint(",deltas_str,");")
  ))
  savepreds <- ls() %>% grep(pattern="pred.",fixed=TRUE,value=TRUE)
  temp <- lapply(as.list(savepreds), function(x){
    save(list=x, 
         file=paste0(rdatafiles,gsub(patt=".",rep="",x=x,fixed=TRUE),".RData"))
    })
  
  fitted.team     <- pred_glm(Xtrain, glmtot)
  fitted.player   <- pred_me(Xtrain, player.mcmc)
  eval(parse(
    text=paste0("fitted.game.", deltas_str," <- pred_disc(Xtrain, game.mcmc.list.", deltas_str, ");\nprint(",deltas_str,");")
  ))
  savefitteds <- ls() %>% grep(pattern="fitted.",fixed=TRUE,value=TRUE)
  temp <- lapply(as.list(savefitteds), function(x){
    save(list=x, 
         file=paste0(rdatafiles,gsub(patt=".",rep="",x=x,fixed=TRUE),".RData"))
  })
  

}else{
  
  prefixes <- c("pred", "fitted")
  suffixes <- paste0(c("team", "player", paste0("game",deltas_str)),".RData")
  loadfiles <- merge(prefixes, suffixes) %>% apply(1, paste0, collapse="") %>% paste0(rdatafiles, .)
  temp <- lapply(as.list(loadfiles), load, envir=globalenv())

}

```


```{r evalfxns, cache=TRUE}

#confusion matrix, classification rate, and brier score
classify <- function(observed = NA, predicted = NA, cutoff = 0.5, gameids = NA){

  eval.list <- list()
  
  if(is.na(gameids[1])){
    gameids <- 1
  }
  
  for(i in 1:n_distinct(gameids)){
    g <- unique(gameids)[i]
    pred <- predicted[gameids == g]
    obs <- observed[gameids == g]
    
    tab <- array(NA, c(2,2))
    rownames(tab) <- c("predicted miss", "predicted make")
    colnames(tab) <- c("observed miss", "observed make")
    pred01 <- ifelse(pred < cutoff, 0, 1)
    
    tab[1,1] <- sum(obs==0 & pred01==0)
    tab[1,2] <- sum(obs==1 & pred01==0)
    tab[2,1] <- sum(obs==0 & pred01==1)
    tab[2,2] <- sum(obs==1 & pred01==1)
    
    rate <- (tab[1,1]+tab[2,2])/length(obs)
    
    eval.list[[as.character(g)]] <-  rate
  }
    
  if(gameids == 1){
    return(rate)
  }else{
    return(eval.list)
  }
}

meansquare <- function(observed = NA, predicted = NA, gameids = NA){

  eval.list <- list()
  
  if(is.na(gameids[1])){
    gameids <- 1
  }
  
  for(i in 1:n_distinct(gameids)){
    g <- unique(gameids)[i]
    pred <- predicted[gameids == g]
    obs <- observed[gameids == g]
    
    MSE <- mean((obs - pred)^2)
    
    eval.list[[as.character(g)]] <-  MSE
  }
    
  if(gameids == 1){
    return(MSE)
  }else{
    return(eval.list)
  }
}

loglikelify <- function(observed = NA, predicted = NA, gameids = NA){

  eval.list <- list()
  
  if(is.na(gameids[1])){
    gameids <- 1
  }
  
  for(i in 1:n_distinct(gameids)){
    g <- unique(gameids)[i]
    pred <- predicted[gameids == g]
    obs <- observed[gameids == g]
    
    #deviance
    loglik <- sum(
      obs*log(pred) + (1-obs)*log((1-pred))
    )
    eval.list[[as.character(g)]] <-  loglik
  }
  
  if(gameids == 1){
    return(loglik)
  }else{
    return(eval.list)
  }
}

#data for calibration plots
calibrate <- function(observed = NA, predicted = NA, nbins = 20, gameids = NA){

  eval.list <- list()
  
  if(is.na(gameids[1])){
    gameids <- 1
  }
  
  for(i in 1:n_distinct(gameids)){
    g <- unique(gameids)[i]
    pred <- predicted[gameids == g]
    obs <- observed[gameids == g]

    if(n_distinct(pred) < nbins){
      calib <- NA
    }else{
      
      predquants <- c(0, quantile(pred, (1:nbins)/nbins))
      predmidpts <- ((predquants + c(0,predquants[-length(predquants)]))/2) %>% '['(-1)
    
      pred1N <- sapply(pred, findInterval, vec=predquants, all.inside=TRUE, rightmost.closed=TRUE, left.open=TRUE)
  
      calibtab <- table(pred1N, obs)
      rownames(calibtab) <- predmidpts
      calib <- calibtab %>% 
        as.data.frame() %>%
        dcast(pred1N ~ obs, value.var = "Freq") %>%
        mutate(p  = `1`/(`0`+`1`),
               pl = p - 2*sqrt(p*(1-p)/(`0` + `1`)),
               pu = p + 2*sqrt(p*(1-p)/(`0` + `1`)),
               binl = predquants[-(nrow(.)+1)],
               binu = predquants[-1])
      colnames(calib) <- c("bin", "obs0", "obs1", "p", "pl", "pu", "binl", "binu")

    }
    
    eval.list[[as.character(g)]] <- calib
  }
  
  if(gameids == 1){
    return(calib)
  }else{
    return(eval.list)
  }
}

plot_calibration <- function(dat, title="Calibration Plot"){
  
  ggplot(data = dat, aes(x=as.numeric(as.character(bin)), y=p, group=0)) + 
    geom_errorbar(aes(ymin=pl, ymax=pu), width=0) + 
    geom_errorbarh(aes(xmin=binl, xmax=binu), height=0) + 
    geom_abline(slope=1, color="red") +
    coord_cartesian(xlim=c(0,1), ylim=c(0,1)) + 
    theme(panel.grid = element_blank()
          #,panel.border = element_blank(), axis.text = element_blank(), axis.ticks =   element_blank()
          ) + 
    theme_bw() + 
    labs(x="Bins",y="Empirical Probability" , title=title)
}

if(!load_chains){
  
  class.team  <- classify(Xtest$result, pred.team)
  class.player <- classify(Xtest$result, pred.player)
  eval(parse(
    text=paste0("class.games <- c(",
           paste0("classify(Xtest$result,pred.game.",deltas_str,collapse="),"),"))"
         )
  ))

  mse.team  <- meansquare(Xtest$result, pred.team)
  mse.player <- meansquare(Xtest$result, pred.player)
  eval(parse(
    text=paste0("mse.games <- c(",
           paste0("meansquare(Xtest$result, pred.game.",deltas_str,collapse="),"),"))"
          )
    ))
  
  lik.team  <- loglikelify(Xtrain$result, fitted.team)
  lik.player <- loglikelify(Xtrain$result, fitted.player)
  eval(parse(
    text=paste0("lik.games <- c(",
           paste0("loglikelify(Xtrain$result, fitted.game.",deltas_str,collapse="),"),"))"
          )
    ))
  eval.df <- data.frame(
    class = c(class.team, class.player, class.games),
    mse = c(mse.team, mse.player, mse.games),
    lik = c(lik.team, lik.player, lik.games),
    type = c("GLM", "Mixed Effects",rep("Discount Likelihood", length(deltas))),
    delta = c(NA, NA, deltas_str),
    k = k0
  )

  save(eval.df, file=paste0(rdatafiles, "evaldf.RData"))
  
}else{
  
  tempenv <- new.env()
  eval.df <- NULL
  
  for(i in 1:k){
    
    load(file = paste0(get_rdatafiles(i), "evaldf.RData"), tempenv)
    eval.df <- rbind(eval.df, tempenv$eval.df)
    
  }
    
  eval.df$k <- as.factor(eval.df$k)
  eval.df$delta <- factor(eval.df$delta,sort(as.character(levels(eval.df$delta)))) #making sure that the deltas are in order

}

df_delta <- eval.df %>% filter(!is.na(delta))
df <- eval.df %>% filter(is.na(delta))
g1 <- ggplot(data=df_delta, aes(x=delta,y=class, col=k)) +
   geom_point() + 
   geom_hline(data=df, aes(yintercept=class, linetype=type, col=k)) +
   theme_bw() + 
   labs(y="Classification Rate \n (0.5 cutoff)", x=expression(delta), linetype = "")

g2 <- ggplot(data=df_delta, aes(x=delta,y=lik, col=k)) +
  geom_point() +
  geom_hline(data=df, aes(yintercept=lik, linetype=type, col=k)) +
  theme_bw() +
  labs(y="Log Likelihood \n", x=expression(delta), linetype = "")

g3 <- ggplot(data=df_delta, aes(x=delta,y=mse, col=k)) +
  geom_point() +
  geom_hline(data=df, aes(yintercept=mse, linetype=type, col=k)) +
  theme_bw() +
  labs(y="Brier Score (MSE)", x=expression(delta), linetype = "")
```

```{r evalplot, fig.cap="Model Evaluation"}
grid.arrange(g1, g2, g3, nrow=2)
```

From Figure \@ref(fig:evalplot), we can observe that the models have different strengths. The discounted likelihood model with the smallest value of $\delta$ consistently has the highest likelihood. However, it does not test as well as the other models in areas of out-of-sample classification rate and Brier score. This suggests that models with smaller values of $\delta$, where the likelihood of an observed shot is more heavily influenced by shots closer to it, may overfit the model to the training data. The generalized linear models perform best in Brier score, but worst in log likelihood. The hierarchical models are about the same, but they have a better log likelihood performance than the GLMs. A model that balances the trade-off between predictive accuracy and likelihood is a discounted likelihood model with $\delta$ = 0.850.

In addition, we can see that the variation in model performance is small. For example, most of the out-of-sample classification rates fall between 0.58 and 0.62. This is within the 95% confidence interval for a random binomial proportion of 0.6 using a sample size of `r n <- k*(length(deltas) + 2); n` (for each combination of `r k`-fold cross-validation and `r length(deltas)+2` different models), which is `r err <- sqrt(0.6*(1-0.6)/n); paste0("(", round(0.6-err, 4), ", " , round(0.6+err, 4), ")")`. Therefore, the evidence that certain models predict better than others is not particularly strong.

For the discounted likelihood model with $\delta$ = 0.850, we build calibration plots to assess how well the estimated probabilities fit the actual proportions. To make these plots, we divide the predicted probabilities into 20 equally-sized bins, then plot these bins on the x-axis versus the proportions of the actual outcomes within the bins on the y-axis. The horizontal bars represent the bin width, and the vertical bars represent a 95% confidence interval of the proportions. The red line of slope 1 represents equality between the bin medians and the empirical probabilities within the bins. In Figure \@ref(fig:caliplot), we present these plots for a full training set and a testing set.

```{r caliplot, fig.cap="Calibration Plots for Discounted Likelihood Model, $\\delta$ = 0.850"}

randtrain <- sample(Xtrain$gameid, 1)
randtest  <- sample(Xtest$gameid, 1)
randtrainrows <- Xtrain$gameid == randtrain
randtestrows  <- Xtest$gameid == randtest

calitrain <- Xtrain$result %>% 
  calibrate(observed=., predicted=fitted.game.850) %>% 
  plot_calibration(., "Training Set")

calitest <- Xtest$result %>% 
  calibrate(observed=., predicted=pred.game.850) %>% 
  plot_calibration(., "Testing Set")

# calitrain1 <- Xtrain$result[randtrainrows] %>% 
#   calibrate(observed=., predicted=fitted.game.850[randtrainrows], nbins=5) %>% 
#   plot_calibration(., "Training Game")
# 
# calitest1 <- Xtest$result[randtestrows] %>% 
#   calibrate(observed=., predicted=pred.game.850[randtestrows]) %>% 
#   plot_calibration(., "Testing Game")

grid.arrange(calitrain, calitest, ncol=2)

```

In this model, we can see that the confidence intervals on the training set all cross the line of slope, which shows that the model reliably estimates the probabilities. In the testing set, however, the predictions only cross this line between about 0.3 and 0.75. In addition, the fact that the bins are so much thinner in the middle shows that the model is not likely to predict values close to 0 or 1.

## Results from Model

To illustrate results from the discounted likelihood model with $\delta$ = 0.850, we plot the location parameters over time for a high-usage player and a low-usage player that play the same position, and for the overall team. **make this sound more academic**

```{r discplot850, fig.cap="Parameters Over Time, $\\delta$ = 0.850"}

idhi <- 842297
idlo <- 551769

plot_time_effect(game.params.850, id=idhi, parameters=c("intH","intA", "r", "theta"), main="High-Usage Player", zlab=c("Intercept (Home)", "Intercept (Away)", "Distance", "Angle"))

plot_time_effect(game.params.850, id=idlo, parameters=c("intH","intA", "r", "theta"), main="Low-Usage Player", zlab=c("Intercept (Home)", "Intercept (Away)", "Distance", "Angle"))

plot_time_effect(game.params.850, id=0, parameters=c("intH","intA", "r", "theta"), main="Team Effects", zlab=c("Intercept (Home)", "Intercept (Away)", "Distance", "Angle"))
```


We can see that the parameters move over time. **elaborate. like the team parameters move the most over time. and the intercepts have wider uncertainty.**
The distance parameter moves the least. This suggests that the ability to shoot long-range shots did not drastically increase or decrease between games in the data.

## Conclusion

The evaluations of the models show that there is not a lot of time-dependency in shooting success rate in this dataset of player-tracking data from the Duke Men's Basketball team. Allowing predictors of shot success to shift based on recent success does not significantly improve the predictive accuracy or the likelihood of a model. However, we do see a systematic improvement in likelihood for smaller discount factors (i.e., more emphasis on recent shots, and therefore support of "streakiness"). 

To account for possible unexplained variation between seasons, and variation introduced from having such a small population of road games, I repeated this analysis on a subset of the data that only consisted of shots from available games in the 2015 season (25 games), and shots from home games (82 games). **results!** (show these results in Appendix?)

## Future Goals

Future goals for this research are to build a better-fitting model to predict basketball shots using more advanced factors that can be approximated from the dataset; possibilities for this include using the distance of the nearest defender as a proxy for defense quality, or using the amount of time a player has played without a substitution or timeout to approximate fatigue.
