[
["introduction.html", "Insert Real Title Introduction", " Insert Real Title Nathaniel Brown May 2018 Introduction In the sport of basketball, points are awarded by the binary event of shooting the ball into the goal. Some factors we consider that may affect the success rate include the location of the shooter, the individual skill of the shooter, whether the shooter is playing on his home court or on an away court, and the shooting success in recent surrounding games. There have been previous studies on how much recent shooting success affects current shooting success, and the results vary. For example, insert examples from literature review. The purpose of this paper is to investigate Bayesian modelling techniques shooting data, and to learn more about time-dependency in shooting data. "],
["1-abstract.html", "Chapter 1 Abstract", " Chapter 1 Abstract This study is an investigation of Bayesian statistical models and analyses for problems arising in shooting a basketball. The dataset is from the Duke Men’s Basketball team’s player-tracking data, which is recorded on the SportVU cameras from STATS, LLC. Goals are to explore, develop, and apply Bayesian models to existing and new data on shooting outcomes. In addition, we want to understand and evaluate questions of inherent random variation, changes over time in shooting performance, and issues related to the “Hot Hand” concept in sports. The models we use to investigate this data are a Bayesian logistic generalized linear model, a hierarchical model with mixed effects on the shooter identity, and a discounted likelihood model that reduces the influence of shots as their time difference from the current shot increases. Our results so far show that the best-fitting model is … "],
["2-litreview.html", "Chapter 2 Literature Review", " Chapter 2 Literature Review 2.0.1 how do I get the full citations to show up and not just last name and year? Gilovich, Vallone, &amp; Tversky (1985) In this research paper from Cognitive Psychology, Thomas Gilovich, Robert Vallone, and Amos Tversky investigate peoples’ belief in the Hot Hand in Basketball. The Hot Hand is the concept that the probability of a success increases for trials that follow a success in a binary sequence; in basketball, these binary events are shot attempts. The methods in this paper include an analysis of shot attempts from the Philadelphia 76ers of the National Basketball Association (NBA) in the 1981 season, analysis of free-throw attempts from the Boston Celtics in the 1981 and 1982 seasons, and a controlled shooting drill using male and female varsity basketball players at Cornell University. Statistical techniques they used to attempt to detect streakiness in the data included Walf-Wolfowitz run tests, autocorrelation tests on consecutive shot attempts, goodness-of-fit tests for the distribution of succesess, and paired t-tests comparing the mean of makes following a make to that of makes following a miss. In addition to this analysis of shooting, this research also contained a survey of basketball fans, that gauged how much people believed success probabilities changed given a success or a failure. The statistical tests did not detect significant evidence supporting the Hot Hand in basketball. The lack of statistical power in Gilovich, Vallone, and Tversky’s frequentist tests motivates the use of Bayesian models in this thesis. Strengths of this paper include the fact that it was one of the first research papers to analyze streakiness in basketball data, and many future papers build off of it. Some weaknesses in this paper are the assumptions it makes in its analysis, such as all shots being independent of each other, and not accounting for shot location. Albert &amp; Williamson (1999) In this paper, Jim Albert attempts to improve upon the low-powered tests of Gilovich, Vallone, and Tverky’s 1985 paper on the Hot Hand. Albert formally defines “streakiness” as the presence of nonstationarity (nonconstant probability between trials) or autocorrelation (sequential dependency). Albert uses Gibbs sampling to approximate posterior densities and to simulate data, then fits two types of models on binary data from baseball and basketball to try to characterize streakiness. He fits an overdispersion model to detect nonstationarity, and a markov switching model to detect sequential dependencies. While he did not uncover strong evidence for the hot hand, one of his takeaways was that overdispersion decreases as time goes on in basketball free throw shooting data. A weakness of this paper is that Albert does not show the results of both the Markov model and the overdispersion model on the same data. We use Albert’s formal definitions of streakiness as well as his motivation for Bayesian models over frequentist tests. Bar-Eli, Avugos, &amp; Raab (2006) This paper is a review of previous hot hand research. It reviews several papers investigating the concept of the “hot hand” in several sports such as basketball, baseball, volleyball, and horeshoe, and other fields such as cognitive science and economics. Bar-Eli, Avugos, and Raab evaluate the datasets, the tests and statistics used, and the conclusions of each study. Overall, the authors summarize 13 papers that oppose the hot hand phenomenon, and 11 that support it; they also acknowledge that the scientific evidence for the hot hand is weaker than the evidence against it, and it is typically more controversial. Instead of just looking to answer whether the hot hand exists, Bar-Eli, Avugos, and Raab also examine how people define a “hot hand”, and the psychological factors behind the belief in it, such as the gambling and game strategy. The strengths of this paper are that it evaluates the strengths and weaknesses of many competing claims, and concisely summarizes the information into a table. A weakness is that they do not make any claim of their own. This paper is useful in this thesis because it describes several data analysis techniques to detect streaks in a binary sequence. Ryan Wetzels (2016) In this research paper, Wetzels conducts a simulation study to investigate the Hot Hand Phenomenon. His analysis consists of calculating Bayes Factors to compare evidence between a Hidden Markov Model with two states and a binomial model with one state. He applies this method to data from basketball foul shots and from visual discernment tests. In the basketball data, he found that Shaquille O’Neal’s free-throws show evidence for a two-state Markov model, while Kobe Bryant’s show more evidence for a one-state binomial model. In the data from the visual discernment tests, he found no strong evidence supporting one model over the other. A strength of this paper is Wetzel’s formal comparison of a Bayesian Markov model to a binomial model. A weakness is that the Bayes Factors only compare evidence between the two models; it does not mean that either model is “good”. We use this paper for the specification of the Hidden Markov Model. Albert (1993) In this paper, Albert uses a Markov switching model to analyze streakiness in baseball pitching data. He concludes that a few players exhibit streakiness, but not enough to reject the null hypothesis. An exploratory technique that we take from this paper is to examine the peaks and valleys in a moving average plot to observe streakiness. A strength of this paper is that Albert controls for situational variables such as home field advantage, the handedness of the pitcher, and the runners on the bases. Albert (2013) In this paper, Albert analyzes streakiness in baseball hitting data. His analysis techniques include using Bayes Factors to compare models of the form \\(f(y_j|p_j) = p_j(1-p_j)^{y_j}, y_j = 0,1,2,...\\); a consistent model with a constant \\(p_j\\), and a streaky model with a varying \\(p_j\\) from a beta distribution. A useful insight that we apply to this paper is the concept that the existence of streakiness depends on the definition of “success” in binary outcome data. He found substantially more evidence for streakiness for when a success was coded as “not a strikeout” instead of a “hit”. Likewise, in this paper we blank. West, Harrison, &amp; Migon (1985) This textbook provides theory, applications, and examples of time series models such Dynamic Generalized Linear Models (DGLMs). More specifically, section 14.4 provides an example of a DGLM for a binomial response variable, which we apply in chapter blank of this research paper. "],
["3-data.html", "Chapter 3 Data 3.1 Description of Dataset 3.2 Data Cleaning 3.3 Exploratory Data Analysis", " Chapter 3 Data (This figure only contains shots that occur in front of the half-court line). Theta has a range of \\(2\\pi\\) radians, but this plot shows that most of the attempts occured within the interval (-\\(\\frac{\\pi}{2}\\), \\(\\frac{\\pi}{2}\\)). This figure also shows the bimodal distribution of shot distance over all players. 3.1 Description of Dataset The data for this analysis comes from SportVU, a player-tracking system from STATS, LLC. that provides precise coordinates for all ten players and the ball at a rate of 25 times per second. The Duke University Men’s Basketball team permitted us to use their SportVU data from the 2014 to 2017 basketball seasons for this project. Since the ability to record this data depends on specialized tracking cameras, Duke does not have this data for every game they play—only home games, and a few road games in arenas that had the techology installed. Therefore, there is a substantial amount of missing data between games. More specifically, between the 2014 and 2017 seasons, the Duke Men’s Basketball team played 147 games; this dataset contains 94 games, with 82 at Duke and 12 on another court. For our analysis, we use the following files for each game: Final Sequence Play-by-Play Optical: This dataset comes in an a semi-structured Extensible Markup Language (XML) file, where there is a unique element for each “event” (an event is a basketball action such as a dribble, pass, shot, foul, etc.). Each event element has attributes describing the type of event, the time of the event, and the player who completed the action. We use these files to uncover when a shot is attempted in a game, who attempted the shot, and the result of the shot attempt. Box Score Optical: We use this dataset to match the names and IDs of players who are in the game. This is also an XML file, with elements corresponding to individual players. These elements contain attributes describing information about the player (e.g. team name, jersey number) and various statistics for the game (e.g. points, assists, distance run). Final Sequence Optical: These XML files contain the locations of all ten players and the ball during precise time intervals within the game. Each timeunit has a unique element, and these elements have attributes describing the locations. We merge this with the Final Sequence Play-by-Play Optical data on the time attribute to obtain the shooter’s location at the moment of a shot attempt. 3.2 Data Cleaning Steps taken to clean the merged shooter IDs with shot locations include standardizing the locations onto a half-court setting (the teams switch sides of the court halfway through every game, which means that we have to flip the coordinates across the middle of the court for half of the data in every game), converting the x-y coordinates to polar coordinates (in the units of feet and radians), and including an indicator for home games. The final dataset had 5467 observations from 31 shooters over 94 games. A summary of the cleaned dataset, and an excerpt: Type Values Extra Details season categorical {2014, …, 2017} gameid categorical NA 94 unique values time continuous NA 13-digit timestamp in milliseconds globalplayerid categorical NA 31 unique values r continuous [0, \\(\\infty\\)) Distance of shot from hoop (feet) theta continuous [-\\(\\pi\\), \\(\\pi\\)] Angle of shot (radians) home categorical {0,1} 1 if shot occured during a home game, 0 otherwise result categorical {0,1} 1 if shot was made, 0 if shot was missed (response) season gameid time globalplayerid r theta home result 2014 201401070173 1389141733839 603106 4.207557 1.0745975 1 1 2014 201401070173 1389141844712 601140 16.653667 1.2972998 1 0 2014 201401070173 1389143172185 696289 18.790131 -0.0580684 1 1 2014 201401070173 1389143196303 601140 23.462949 0.9538861 1 1 2014 201401070173 1389143220261 756880 6.536497 0.0696471 1 0 3.3 Exploratory Data Analysis The following exploratory plots examine how consistent the probability of a made shot is, using a loess smooth curve on the binary outcomes. We present these smoothed plots for four high-usage basketball players at Duke University between the 2013-2014 and 2016-2017 seasons, and we leave the others in Appendix number. Each plot represents a single player’s ordered shooting outcomes for a single season. These plots do not account for the amount of time in between shots, but simply shot order and outcome. We can see that the plots vary in the consistency of their made shots, since they all contain spikes and trends. For example, the third plot initially has a very high success rate, which quickly falls to the middle after about thirty shot attempts, and the second plot has a noticeable upward trend in shot success beginning around shot number one hundred fifty. We investigate the shooting outcomes using Bayesian models, and present the results in the next section. "],
["4-models.html", "Chapter 4 Models &amp; Analysis 4.1 Description of Models 4.2 Analysis", " Chapter 4 Models &amp; Analysis 4.1 Description of Models For our models, we consider the shot location, the shooter identity, a home court indicator, and a player’s shooting success rate in nearby games as factors that can affect a shot outcome. In every model, we use the Just Another Gibbs Sampler library in R (R2jags). Each model is based off of a logistic regression model that provides the posterior distribution of the shot location parameters (distance and angle). The models do not account for covariance between these predictors. We expand upon this model by adding mixed effects and discounted likelihood models to control for shooter identity and game identity, respectively. These models will show us how consistent the shot location parameters are between shooters and between games. In our Gibbs Samplers, our priors are set using the corresponding Maximum Likelihood Estimates for the first four games in the dataset, and we initialize our Monte Carlo Markov Chains using values of 0 for all means, and 1 for all variances. The R2jags code used to build these models, as well as some diagnostic plots, can be found in Appendices number and number, respectively. 4.1.1 Generalized Linear Model First, we build a logistic regression model of the following form: \\[ \\text{log} \\left( \\frac{p}{1-p} \\right) = \\beta_{\\text{int}} + X_{\\text{r}}\\beta_{\\text{r}} + X_{\\theta}\\beta_{\\theta} + X_{\\text{H}}\\beta_{\\text{H}} + \\epsilon \\] \\[ \\epsilon \\sim N(0, \\sigma^2) \\] In this model, \\(X\\) refers to the data, \\(\\beta\\) is the estimated pareter from the model, and \\(\\epsilon\\) is random error. The subscripts \\(\\textit{int}\\), \\(\\textit{r}\\), \\(\\theta\\), and \\(\\textit{H}\\) respectively refer to the intercept, the log-distance of the shot, the angle of the shot, and whether shot was taken on Duke’s home court or another gym. 4.1.2 Hierarchical Generalized Linear Model Our second model is a hierarchical model, with random effects on the players. These random effects occur for each of the four parameters of interest—the intercept, the home effect, the distance effect, and the angle effect. Each indivual player’s parameter values are sampled from a Normal distribution cntered at the population values. The parameters for players without a lot of shot attemps are shrunk towards the population means. \\[ \\text{log} \\left( \\frac{p}{1-p} \\right) = \\beta_{\\text{int, j}} + X_{\\text{r}}\\beta_{\\text{r, j}} + X_{\\theta}\\beta_{\\theta, j} + X_{\\text{H}}\\beta_{\\text{H, j}} + \\epsilon \\] \\[ \\beta_{\\text{int, j}} \\sim N(\\beta_{\\text{int}}, \\tau^2_{\\text{int}}) \\] \\[ \\beta_{\\text{r, j}} \\sim N(\\beta_{\\text{r}}, \\tau^2_{\\text{r}}) \\] \\[ \\beta_{\\theta, j} \\sim N(\\beta_{\\theta}, \\tau^2_{\\theta}) \\] \\[ \\beta_{\\text{H, j}} \\sim N(\\beta_{\\text{H}}, \\tau^2_{\\text{H}}) \\] \\[ \\epsilon \\sim N(0, \\sigma^2) \\] 4.1.3 Discounted Likelihood Hierarchical Model In the discounted likelihood models, the likelihood of a single observation is more heavily influenced by the observations close to it than the observations far away from it. We measure the “distance” between observations by the number of games between them; a shot attempt that occurs in the next or the preceding game will influence the likelihood of the observation more than a shot that occurs two games away. To build these models, we applied the “ones trick” in the R2jags library. See Appendix number for more details. This model also contains the same random effects as the above model. \\[ \\text{log} \\left( \\frac{p}{1-p} \\right) = \\beta_{\\text{int, j}} + X_{\\text{r}}\\beta_{\\text{r, j}} + X_{\\theta}\\beta_{\\theta, j} + X_{\\text{H}}\\beta_{\\text{H, j}} + \\epsilon \\] \\[ L(\\text{p}) = \\Pi_{\\text{i=1}}^\\text{n}(\\text{p}_i)^{1-\\text{y}_i} (1 - \\text{p}_i)^{1-\\text{y}_i} = \\mu \\] \\[ \\pi = \\mu^{\\delta_i} \\] \\[ \\delta_i = \\Delta^{|g_i - g_0|} \\] \\[ \\beta_{\\text{int, j}} \\sim N(\\beta_{\\text{int}}, \\tau^2_{\\text{int}}) \\] \\[ \\beta_{\\text{r, j}} \\sim N(\\beta_{\\text{r}}, \\tau^2_{\\text{r}}) \\] \\[ \\beta_{\\theta, j} \\sim N(\\beta_{\\theta}, \\tau^2_{\\theta}) \\] \\[ \\beta_{\\text{H, j}} \\sim N(\\beta_{\\text{H}}, \\tau^2_{\\text{H}}) \\] \\[ \\epsilon \\sim N(0, \\sigma^2) \\] In this model, \\(\\mu\\) represents the likelihood of the binomial events \\(y_i\\), while \\(\\pi\\) represents the discounted likelihood. These equations show us that the contribution of an observed shot (in game \\(g_0\\)) to the likelihood of the current shot (in game \\(g_i\\)) decreases as the distance between the observations increases, and as \\(\\Delta\\) decreases. In a model with \\(\\Delta\\) = 0, then only shots taken in the same game as \\(g_i\\) can contribute to the likelihood, while \\(\\Delta\\) = 1 corresponds to a model with no discounting. This model specification results in an MCMC chain for each combination of \\(g_0\\) and \\(\\Delta\\). The code for each of these models is located in Appendix number. 4.2 Analysis 4.2.1 Generalized Linear Model In our logistic regression model, we only look at shot location and the home court indicator as predictors for the shot outcome. To look at these effects for particular players, we simply subset the dataset to shots attempted by that player before running the Gibbs Sampler. The results of the 95% credible intervals are reported for the same four players, in the same order as they were presented in the Exploratory Data Analysis section. In the generalized linear model, the intercepts correspond to the log-odds of making a shot when angle is zero (the middle of the court) and the log distance is zero (one foot away from the hoop). From these plots, we see that the team-wide 95% credible interval of the angle effect contains zero, and it is therefore probably not predictive of a made shot. The average distance effect shows us that the log-odds of a made shot decrease by \\(\\beta_r =\\) 0.5372 as the log distance increases by one unit and the other predictors remain constant. In the probability scale, this decrease is: \\[ \\frac{\\text{e}^{\\beta_r}}{1 + \\text{e}^{\\beta_r}} \\] which is equal to 0.3689. We also see that the 95% credible interval on the effect of distance is completely negative, which follows the intuitive idea that the probability of a made shot significantly decreases as distance from the basket increases. The intercepts show us that there is not a substantial diference in baseline shooting performance between home games and away games. 4.2.2 Hierarchical Generalized Linear Model In this hierarchical model, we add random effects to allow the parameters to vary for each player in the dataset. We present the results below using densities of how the four players of interest compare to the population distribution, and by using contour plots that illustrate each player’s probability of a made shot given their location on the court for a game at home. These hierarchical model results show us characteristics of our four high-usage players of interest compared to the population of players in the dataset. For example, the intercept plots show us that Player 2 is excellent at finishing under baseline conditions, but he also has a steeper-than-averge drop in his odds of scoring as his distance from the basket increases. This means that most of his scoring occurs close to the basket. We can also see that Player 1 strongly increases his odds of scoring when his angle is negative, which corresponds to the left side of the basket. In the figures below, we have contour plots showing players’ expected field goal percentages at different locations on the court. Between our four players of interest, we can observe how Player 1 is more effective on the left side of the basket than the others, and how Player 2 has the darkest overall contour plot, suggesting he has the highest probability of scoring among those four. 4.2.3 Discounted Likelihood Hierarchical Model The values of \\(\\Delta\\) that we use to fit the discounted likelihood models are 0.750, 0.800, 0.850, 0.900, 0.950, and 0.999. We also build an MCMC chain to estimate the posterior using every game as \\(g_0\\). We calculate predictions and fitted values for a particular shot in game \\(g_i\\) using the posterior median of the MCMC chain where \\(g_i\\) is \\(g_0\\). If models with larger values of \\(\\Delta\\) best fit the data, this suggests that shooting success is consistent throughout a career. If smaller values of \\(\\Delta\\) are more likely in the data, however, then we can assume there is a substantial amount of time variation in the data on the game level. The plots above illustrate how the intercepts change for players over time. These plots illustrate that smaller values of \\(\\Delta\\) allow for greater variation in parameter values over time. "],
["5-disc.html", "Chapter 5 Discussion 5.1 Evaluation of Models 5.2 Conclusion 5.3 Future Goals", " Chapter 5 Discussion Summarize and reflect upon results 5.1 Evaluation of Models To evaluate these models, we use 5-fold cross-validation. This process used as many as 20 simultaneous RStudio Pro servers from the Duke University Statistical Science Department. In each train-test split, we evaluate a model’s out-of-sample classification rate (using a cutoff probability of 0.5), Brier score (mean squared error), and log likelihood. The predictions and fitted values are obtained using MCMC averages over ___. The results are plotted below: From these plots, we can observe that all of the models have different strengths. The discounted likelihood model with the smallest \\(\\delta\\) consistently has the highest likelihood. However, it does not test as well as the other models in areas of out-of-sample classification rate and Brier score. This suggests that a smaller value of \\(\\delta\\) leads to overfitting the model to the training data, since the likelihood of a particular shot is only influenced by shots close to it. talk about how they are similar though. like ranges of classification rates To account for possible unexplained variation between seasons, and variation introduced from having such a small population of road games, I repeated this analysis on a subset of the data that only consisted of shots from available games in the 2015 season (25 games), and shots from home games (82 games). results! (show these results in Appendix?) For the remainder of this paper, we will focus on the model with delta = 0.850. 5.2 Conclusion The results of this paper show that there is not a lot of time-dependency in shooting success rate in this dataset of player-tracking data from the Duke Men’s Basketball team. The fact that the models with higher values of 5.3 Future Goals Future goals for this research are to build a better-fitting model to predict basketball shots using more advanced factors that can be approximated from the dataset; possibilities for this include using the distance of the nearest defender as a proxy for defense quality, or using the amount of time a player has played without a substitution or timeout to approximate fatigue. If you feel it necessary to include an appendix, it goes here. # The First Appendix This first appendix includes all of the R chunks of code that were hidden throughout the document (using the `include = FALSE` chunk tag) to help with readibility and/or setup. **In the main Rmd file** ```r # This chunk ensures that the thesisdowndss package is # installed and loaded. This thesisdowndss package includes # the template files for the thesis. if(!require(devtools)) install.packages(\"devtools\", repos = \"http://cran.rstudio.com\") if(!require(thesisdowndss)) devtools::install_github(\"mine-cetinkaya-rundel/thesisdowndss\") library(thesisdowndss) ``` **In Chapter \\@ref(ref-labels):** # The Second Appendix, for Fun --> "],
["A-appendix-1-code.html", "A Appendix 1: Code A.1 Generalized Linear Model A.2 Hierarchical Generalized Linear Model A.3 Discounted Likelihood Hierarchical Model", " A Appendix 1: Code A.1 Generalized Linear Model Xtrainsub &lt;- Xtrain %&gt;% filter(as.integer(as.factor(gameid)) &lt; 5) priormod &lt;- glm(formula = result ~ log(r) + theta, data = Xtrainsub, family = &quot;binomial&quot;) mu0r &lt;- summary(priormod)[[&quot;coefficients&quot;]][&quot;log(r)&quot;,&quot;Estimate&quot;] mu0theta &lt;- summary(priormod)[[&quot;coefficients&quot;]][&quot;theta&quot;,&quot;Estimate&quot;] fit_glm &lt;- function(dat, S = 10000, B = 500){ model.glm &lt;- function(){ # Liklihood function (for N observations) for(i in 1:N){ logit(prob[i]) &lt;- beta_int*int[i] + beta_home*home[i] + beta_r*logr[i] + beta_theta*theta[i] result[i] ~ dbern(prob[i]) } # Priors # we expect less variation in the distance parameter, # because shot success rate should get worse # as distance increases under baseline circumstances. beta_int ~ dnorm(0, 0.1) beta_home ~ dnorm(0, 0.1) beta_r ~ dnorm(mu0r, 0.01) beta_theta ~ dnorm(mu0theta, 0.1) } datlist.glm &lt;- list( int = rep(1, nrow(dat)), logr = log(dat$r), theta = dat$theta, result = dat$result, home = dat$home, N = nrow(dat), mu0r = mu0r, mu0theta = mu0theta ) params.glm &lt;- c(&quot;beta_int&quot;, &quot;beta_home&quot;, &quot;beta_r&quot;, &quot;beta_theta&quot;) initslist &lt;- list(list(&quot;beta_int&quot;=0, &quot;beta_r&quot;=0, &quot;beta_theta&quot;=0, &quot;beta_home&quot;=0)) sim &lt;- jags(data = datlist.glm, n.chains = 1, n.iter = S, n.burnin = B, n.thin = 1, inits=initslist, parameters.to.save = params.glm, model.file=model.glm ) sim.mcmc &lt;- as.data.frame(as.mcmc(sim)[[1]]) # Changing from a baseline mean + a shift amount # to two different means based on the type of game. sim.mcmc &lt;- sim.mcmc %&gt;% mutate(beta_intA = beta_int, beta_intH = beta_int + beta_home) %&gt;% select(beta_intA, beta_intH, beta_r, beta_theta) return(sim.mcmc) } A.2 Hierarchical Generalized Linear Model Xtrainsub &lt;- Xtrain %&gt;% filter(as.integer(as.factor(gameid)) &lt; 5) priormod &lt;- glm(formula = result ~ log(r) + theta, data = Xtrainsub, family = &quot;binomial&quot;) mu0r &lt;- summary(priormod)[[&quot;coefficients&quot;]][&quot;log(r)&quot;,&quot;Estimate&quot;] mu0theta &lt;- summary(priormod)[[&quot;coefficients&quot;]][&quot;theta&quot;,&quot;Estimate&quot;] fit_players &lt;- function(dat = NA, S = 10000, B = 500){ model.player &lt;- function(){ # Likelihood function for N observations for(i in 1:N){ # the parameters now vary by player id. logit(prob[i]) &lt;- beta_int[player[i]]*int[i] + beta_home[player[i]]*home[i] + beta_r[player[i]]*logr[i] + beta_theta[player[i]]*theta[i] result[i] ~ dbern(prob[i]) } # Priors for(j in 1:M){ beta_int[j] ~ dnorm(beta_int0,tau_int) beta_home[j] ~ dnorm(beta_home0, tau_int) beta_r[j] ~ dnorm(beta_r0,tau_r) beta_theta[j] ~ dnorm(beta_theta0,tau_theta) } # Hyperpriors beta_int0 ~ dnorm(0, 0.1) beta_home0 ~ dnorm(0, 0.1) beta_r0 ~ dnorm(mu0r, 0.01) beta_theta0 ~ dnorm(mu0theta, 0.1) tau_int ~ dgamma(10, 100) tau_r ~ dgamma(10, 0.2) tau_theta ~ dgamma(10, 10) } datlist.player &lt;- list( logr = log(dat$r), theta = dat$theta, home = dat$home, result = dat$result, player = as.integer(as.factor(dat$globalplayerid)), N = nrow(dat), int = rep(1, nrow(dat)), M = n_distinct(dat$globalplayerid), mu0r = mu0r, mu0theta = mu0theta ) # we want posteriors for the overall effects # and for the individual player effects params &lt;- c(&quot;beta_int&quot;, &quot;beta_home&quot;, &quot;beta_r&quot;, &quot;beta_theta&quot;, &quot;beta_int0&quot;, &quot;beta_home0&quot;, &quot;beta_r0&quot;, &quot;beta_theta0&quot;) M &lt;- datlist.player$M initslist &lt;- list( list(&quot;beta_int&quot;=rep(0,M), &quot;beta_home&quot;=rep(0,M), &quot;beta_r&quot;=rep(0,M), &quot;beta_theta&quot;=rep(0,M), &quot;beta_int0&quot;=0, &quot;beta_home0&quot;=0, &quot;beta_r0&quot;=0, &quot;beta_theta0&quot;=0, &quot;tau_int&quot;=1, &quot;tau_r&quot;=1, &quot;tau_theta&quot;=1 )) sim.player &lt;- jags(data = datlist.player, n.iter = S, n.chains = 1, n.burnin = B, n.thin = 1, inits=initslist, parameters.to.save = params, model.file=model.player ) sim.mcmc.player &lt;- as.data.frame(as.mcmc(sim.player)[[1]]) # Changing from a baseline mean + a shift amount # to two different means based on the type of game. hometext &lt;- paste0(&quot;`beta_intH[&quot;,1:M,&quot;]` = `beta_int[&quot;,1:M,&quot;]` + `beta_home[&quot;,1:M,&quot;]`&quot;, collapse=&quot;,\\n&quot;) awaytext &lt;- paste0(&quot;`beta_intA[&quot;,1:M,&quot;]` = `beta_int[&quot;,1:M,&quot;]`&quot;, collapse=&quot;,\\n&quot;) sim.mcmc.player &lt;- eval(parse(text= paste0(&quot;sim.mcmc.player %&gt;% mutate(&quot;,hometext,&quot;, beta_intH0 = beta_int0 + beta_home0)&quot;, &quot; %&gt;% rename(&quot;,awaytext,&quot;, beta_intA0 = beta_int0)&quot; ))) %&gt;% select(grep(&quot;(beta_int)|(beta_theta)|(beta_r)&quot;,names(.))) sim.mcmc.player &lt;- sim.mcmc.player[ ,order(colnames(sim.mcmc.player))] # Renaming mixed effects columns from default factor levels (integers) to the corresponding player ids factorids &lt;- str_extract_all(names(sim.mcmc.player), &quot;[[:digit:]]+&quot;) %&gt;% as.numeric() fids &lt;- data.frame(factorid = factorids, order = 1:length(factorids)) datmap &lt;- dat %&gt;% mutate(factorid = as.integer(as.factor(globalplayerid))) %&gt;% select(globalplayerid, factorid) gameids &lt;- merge(datmap, fids, all.x=FALSE,all.y=TRUE) %&gt;% unique() %&gt;% mutate(globalplayerid = ifelse(is.na(globalplayerid),0,globalplayerid)) %&gt;% arrange(order) names(sim.mcmc.player) &lt;- str_replace_all(names(sim.mcmc.player), &quot;[[:digit:]]+&quot;, as.character(gameids$globalplayerid)) - return(sim.mcmc.player) } A.3 Discounted Likelihood Hierarchical Model fit_game &lt;- function(dat = NA, g0 = NA, Delta = NA, S = 10000, B = 500){ model.game &lt;- function(){ for(i in 1:N){ delta[i] &lt;- Delta^abs(games[i]-g0) # Delta = discount rate for game g relative to anchor game g0 # player-level random effects logit(prob[i]) &lt;- beta_int[player[i]]*int[i] + beta_home[player[i]]*home[i] + beta_r[player[i]]*logr[i] + beta_theta[player[i]]*theta[i] # Likelihood function p1[i] &lt;- prob[i]^result[i] p2[i] &lt;- (1-prob[i])^(1-result[i]) # Discounted likelihood function p[i] &lt;- (p1[i] * p2[i])^delta[i] # defines correct discounted likelihood function y[i] ~ dbern(p[i]) # result = the actual outcome # prob = the actual likelihood # p = discounted likelihood # y = artificial &quot;ones trick&quot; outcomes } # Priors for(j in 1:M){ beta_int[j] ~ dnorm(beta_int0,tau_int) beta_home[j] ~ dnorm(beta_home0, tau_int) beta_r[j] ~ dnorm(beta_r0,tau_r) beta_theta[j] ~ dnorm(beta_theta0,tau_theta) } # Hyperoriors beta_int0 ~ dnorm(0, 0.1) beta_home0 ~ dnorm(0, 0.1) beta_r0 ~ dnorm(mu0r, 0.01) beta_theta0 ~ dnorm(mu0theta, 0.1) tau_int ~ dgamma(10, 100) tau_r ~ dgamma(10, 0.2) tau_theta ~ dgamma(10, 10) } datlist.game &lt;- list( int = rep(1, nrow(dat)), logr = log(dat$r), theta = dat$theta, result = dat$result, home = dat$home, player = as.integer(as.factor(dat$globalplayerid)), N = nrow(dat), M = n_distinct(dat$globalplayerid), mu0r = mu0r, mu0theta = mu0theta, Delta = Delta, games = as.integer(as.factor(dat$gameid)), g0 = g0, y = rep(1, nrow(dat)) ) params &lt;- c(&quot;beta_int&quot;, &quot;beta_r&quot;, &quot;beta_home&quot;, &quot;beta_theta&quot;, &quot;beta_int0&quot;, &quot;beta_home0&quot;, &quot;beta_r0&quot;, &quot;beta_theta0&quot;) M &lt;- n_distinct(dat$globalplayerid) initslist &lt;- list(list(&quot;beta_int&quot;=rep(0,M), &quot;beta_r&quot;=rep(0,M), &quot;beta_theta&quot;=rep(0,M), &quot;beta_int0&quot;=0, &quot;beta_r0&quot;=0, &quot;beta_theta0&quot;=0, &quot;tau_int&quot;=1, &quot;tau_r&quot;=1, &quot;tau_theta&quot;=1 )) sim.game &lt;- jags(data = datlist.game, n.iter = S, n.chains = 1, n.burnin = B, n.thin = 1, inits = initslist, parameters.to.save = params, model.file=model.game ) sim.mcmc.game &lt;- as.data.frame(as.mcmc(sim.game)[[1]]) # Changing from a baseline mean + a shift amount # to two different means based on the type of game. hometext &lt;- paste0(&quot;`beta_intH[&quot;,1:M,&quot;]` = `beta_int[&quot;,1:M,&quot;]` + `beta_home[&quot;,1:M,&quot;]`&quot;, collapse=&quot;,\\n&quot;) awaytext &lt;- paste0(&quot;`beta_intA[&quot;,1:M,&quot;]` = `beta_int[&quot;,1:M,&quot;]`&quot;, collapse=&quot;,\\n&quot;) sim.mcmc.game &lt;- eval(parse(text= paste0(&quot;sim.mcmc.game %&gt;% mutate(&quot;,hometext,&quot;, beta_intH0 = beta_int0 + beta_home0)&quot;, &quot; %&gt;% rename(&quot;,awaytext,&quot;, beta_intA0 = beta_int0)&quot;) )) %&gt;% select(grep(&quot;(beta_int)|(beta_theta)|(beta_r)&quot;,names(.))) # Renaming mixed effects columns from default factor levels (integers) to the corresponding player ids sim.mcmc.game &lt;- sim.mcmc.game[ , order(colnames(sim.mcmc.game))] factorids &lt;- str_extract_all(names(sim.mcmc.game), &quot;[[:digit:]]+&quot;) %&gt;% as.numeric() fids &lt;- data.frame(factorid = factorids, order = 1:length(factorids)) datmap &lt;- dat %&gt;% mutate(factorid = as.integer(as.factor(globalplayerid))) %&gt;% select(globalplayerid, factorid) gameids &lt;- merge(datmap, fids, all.x=FALSE,all.y=TRUE) %&gt;% unique() %&gt;% mutate(globalplayerid = ifelse(is.na(globalplayerid),0,globalplayerid)) %&gt;% arrange(order) names(sim.mcmc.game) &lt;- str_replace_all(names(sim.mcmc.game), &quot;[[:digit:]]+&quot;, as.character(gameids$globalplayerid)) return(sim.mcmc.game) } "],
["B-appendix-2-diagnostic-plots.html", "B Appendix 2: Diagnostic Plots B.1 Generalized Linear Model:", " B Appendix 2: Diagnostic Plots B.1 Generalized Linear Model: "],
["C-appendix-3-reproducing-models.html", "C Appendix 3: Reproducing Models C.1 Home Games Only C.2 2015 Season Only", " C Appendix 3: Reproducing Models C.1 Home Games Only C.2 2015 Season Only "],
["references.html", "References", " References "]
]
