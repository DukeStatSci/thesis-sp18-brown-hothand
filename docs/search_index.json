[
["index.html", "My Final College Paper Introduction", " My Final College Paper Nathaniel Brown May 2018 Introduction In the sport of basketball, points are awarded by the binary event of shooting the ball into the goal. Some factors we consider that may affect the success rate include the location of the shooter, the individual skill of the shooter, whether the shooter is playing on his home court or on an away court, and the shooting success in recent surrounding games. There have been previous studies on how much recent shooting success affects current shooting success, and the results vary. For example, insert examples. The purpose of this paper is to investigate Bayesian modelling techniques shooting data, and to learn more about time-dependency in shooting data. "],
["1-abstract.html", "Chapter 1 Abstract", " Chapter 1 Abstract This study is an investigation of Bayesian statistical models and analyses for problems arising in shooting a basketball. The dataset is from the Duke Men’s Basketball team’s player-tracking data, which is recorded on the SportVU cameras from STATS, LLC. Goals are to explore, develop, and apply Bayesian models to existing and new data on shooting outcomes. In addition, we want to understand and evaluate questions of inherent random variation, changes over time in shooting performance, and issues related to the “Hot Hand” concept in sports. The models we use to investigate this data are a Bayesian logistic generalized linear model, a hierarchical model with mixed effects on the shooter identity, and a discounted likelihood model that reduces the influence of shots as their time difference from the current shot increases. Our results so far show that the best-fitting model is … "],
["2-litreview.html", "Chapter 2 Literature Review", " Chapter 2 Literature Review 2.0.1 how do I get the full citations to show up and not just last name and year? Gilovich, Vallone, &amp; Tversky (1985) In this research paper from Cognitive Psychology, Thomas Gilovich, Robert Vallone, and Amos Tversky investigate peoples’ belief in the Hot Hand in Basketball. The Hot Hand is the concept that the probability of a success increases for trials that follow a success in a binary sequence; in basketball, these binary events are shot attempts. The methods in this paper include an analysis of shot attempts from the Philadelphia 76ers of the National Basketball Association (NBA) in the 1981 season, analysis of free-throw attempts from the Boston Celtics in the 1981 and 1982 seasons, and a controlled shooting drill using male and female varsity basketball players at Cornell University. Statistical techniques they used to attempt to detect streakiness in the data included Walf-Wolfowitz run tests, autocorrelation tests on consecutive shot attempts, goodness-of-fit tests for the distribution of succesess, and paired t-tests comparing the mean of makes following a make to that of makes following a miss. In addition to this analysis of shooting, this research also contained a survey of basketball fans, that gauged how much people believed success probabilities changed given a success or a failure. The statistical tests did not detect significant evidence supporting the Hot Hand in basketball. The lack of statistical power in Gilovich, Vallone, and Tversky’s frequentist tests motivates the use of Bayesian models in this thesis. Strengths of this paper include the fact that it was one of the first research papers to analyze streakiness in basketball data, and many future papers build off of it. Some weaknesses in this paper are the assumptions it makes in its analysis, such as all shots being independent of each other, and not accounting for shot location. Albert &amp; Williamson (1999) In this paper, Jim Albert attempts to improve upon the low-powered tests of Gilovich, Vallone, and Tverky’s 1985 paper on the Hot Hand. Albert formally defines “streakiness” as the presence of nonstationarity (nonconstant probability between trials) or autocorrelation (sequential dependency). Albert uses Gibbs sampling to approximate posterior densities and to simulate data, then fits two types of models on binary data from baseball and basketball to try to characterize streakiness. He fits an overdispersion model to detect nonstationarity, and a markov switching model to detect sequential dependencies. While he did not uncover strong evidence for the hot hand, one of his takeaways was that overdispersion decreases as time goes on in basketball free throw shooting data. A weakness of this paper is that Albert does not show the results of both the Markov model and the overdispersion model on the same data. We use Albert’s formal definitions of streakiness as well as his motivation for Bayesian models over frequentist tests. Bar-Eli, Avugos, &amp; Raab (2006) This paper is a review of previous hot hand research. It reviews several papers investigating the concept of the “hot hand” in several sports such as basketball, baseball, volleyball, and horeshoe, and other fields such as cognitive science and economics. Bar-Eli, Avugos, and Raab evaluate the datasets, the tests and statistics used, and the conclusions of each study. Overall, the authors summarize 13 papers that oppose the hot hand phenomenon, and 11 that support it; they also acknowledge that the scientific evidence for the hot hand is weaker than the evidence against it, and it is typically more controversial. Instead of just looking to answer whether the hot hand exists, Bar-Eli, Avugos, and Raab also examine how people define a “hot hand”, and the psychological factors behind the belief in it, such as the gambling and game strategy. The strengths of this paper are that it evaluates the strengths and weaknesses of many competing claims, and concisely summarizes the information into a table. A weakness is that they do not make any claim of their own. This paper is useful in this thesis because it describes several data analysis techniques to detect streaks in a binary sequence. Ryan Wetzels (2016) In this research paper, Wetzels conducts a simulation study to investigate the Hot Hand Phenomenon. His analysis consists of calculating Bayes Factors to compare evidence between a Hidden Markov Model with two states and a binomial model with one state. He applies this method to data from basketball foul shots and from visual discernment tests. In the basketball data, he found that Shaquille O’Neal’s free-throws show evidence for a two-state Markov model, while Kobe Bryant’s show more evidence for a one-state binomial model. In the data from the visual discernment tests, he found no strong evidence supporting one model over the other. A strength of this paper is Wetzel’s formal comparison of a Bayesian Markov model to a binomial model. A weakness is that the Bayes Factors only compare evidence between the two models; it does not mean that either model is “good”. We use this paper for the specification of the Hidden Markov Model. Albert (1993) In this paper, Albert uses a Markov switching model to analyze streakiness in baseball pitching data. He concludes that a few players exhibit streakiness, but not enough to reject the null hypothesis. An exploratory technique that we take from this paper is to examine the peaks and valleys in a moving average plot to observe streakiness. A strength of this paper is that Albert controls for situational variables such as home field advantage, the handedness of the pitcher, and the runners on the bases. Albert (2013) In this paper, Albert analyzes streakiness in baseball hitting data. His analysis techniques include using Bayes Factors to compare models of the form \\(f(y_j|p_j) = p_j(1-p_j)^{y_j}, y_j = 0,1,2,...\\); a consistent model with a constant \\(p_j\\), and a streaky model with a varying \\(p_j\\) from a beta distribution. A useful insight that we apply to this paper is the concept that the existence of streakiness depends on the definition of “success” in binary outcome data. He found substantially more evidence for streakiness for when a success was coded as “not a strikeout” instead of a “hit”. Likewise, in this paper we blank. West, Harrison, &amp; Migon (1985) This textbook provides theory, applications, and examples of time series models such Dynamic Generalized Linear Models (DGLMs). More specifically, section 14.4 provides an example of a DGLM for a binomial response variable, which we apply in chapter blank of this research paper. "],
["3-data.html", "Chapter 3 Data", " Chapter 3 Data The data for this analysis comes from SportVU, a player-tracking system from STATS, LLC. that provides precise coordinates for all ten players and the ball at a rate of 25 times per second. The Duke University Men’s Basketball team permitted us to use their SportVU data from the 2014 to 2017 basketball seasons for this project. Since the ability to record this data depends on specialized tracking cameras, Duke does not have this data for every game they play—only home games, and a few road games in arenas that had the techology installed. Therefore, there is a substantial amount of missing data between games. For our analysis, we use the following files for each game: Final Sequence Play-by-Play Optical: This dataset comes in an a semi-structured Extensible Markup Language (XML) file, where there is a unique element for each “event” (an event is a basketball action such as a dribble, pass, shot, foul, etc.). Each event element has attributes describing the type of event, the time of the event, and the player who completed the action. We use these files to uncover when a shot is attempted in a game, who attempted the shot, and the result of the shot attempt. Box Score Optical: We use this dataset to match the names and IDs of players who are in the game. This is also an XML file, with elements corresponding to individual players. These elements contain attributes describing information about the player (e.g. team name, jersey number) and various statistics for the game (e.g. points, assists, distance run). Final Sequence Optical: These XML files contain the locations of all ten players and the ball during precise time intervals within the game. Each timeunit has a unique element, and these elements have attributes describing the locations. We merge this with the Final Sequence Play-by-Play Optical data on the time attribute to obtain the shooter’s location at the moment of a shot attempt. Data Cleaning Steps taken to clean the merged shooter IDs with shot locations include standardizing the locations onto a half-court setting (the teams switch sides of the court halfway through every game, which means that we have to flip the coordinates across the middle of the court for half of the data in every game), converting the x-y coordinates to polar coordinates (in the units of feet and radians), and including an indicator for home games. The final dataset had 5467 observations from 31 shooters over 94 games. The cleaned dataset is summarized below: "],
["4-proc.html", "Chapter 4 Procedure", " Chapter 4 Procedure Using the time-stamped sequence of shot locations and binary outcomes, we fit Bayesian logistic generalized linear models, hierarchichal models, and discounted likelihood models. The discounted likelihood models allow us to observe how strongly shot success rate in the current game is affected by performance in recent games. We deal with the missing data by comparing the results over all games to the results on only home games, since no home games are missing in the dataset. The formal Bayesian analysis allows us to produce full quantified inferences on these patterns over time, with probabilistic summaries of the between-game outcomes. For each provided game, we will analyze player shooting tendencies and outcomes, which provides understanding of inherent variability (or “randomness”) for the players, and formal assessments of differences in patterns game-to-game. "],
["5-EDA.html", "Chapter 5 Exploratory Data Analysis", " Chapter 5 Exploratory Data Analysis 5.0.0.1 Exploratory Data Analysis The following exploratory plots examine how consistent the probability of a made shot is, using a loess smooth curve on the binary outcomes. We present these smoothed plots for four high-usage basketball players at Duke University between the 2013-2014 and 2016-2017 seasons, and we leave the others in Appendix number. Each plot represents a single player’s ordered shooting outcomes for a single season. These plots do not account for the amount of time in between shots, but simply shot order and outcome. load(file=&quot;../rdatafiles/Xtot.RData&quot;) id1 &lt;- 887665 id2 &lt;- 842301 id3 &lt;- 603106 id4 &lt;- 601140 playerseasons &lt;- matrix( c(id1, 2016, id2, 2015, id3, 2014, id4, 2015 ),ncol=2,byrow = TRUE ) colnames(playerseasons) &lt;- c(&quot;globalplayerid&quot; ,&quot;season&quot;) plotshottime &lt;- function(playerid, season, playernum){ Xtot_sub &lt;- Xtot[Xtot$globalplayerid==playerid &amp; Xtot$season==season,] par(las=1) if(nrow(Xtot_sub) &lt; 10){ # ggplot(data=NULL) + labs(title = &quot;Not Enough Data&quot;) + theme_bw() plot(0,0,type=&quot;n&quot;, yaxt=&quot;n&quot;,xaxt=&quot;n&quot;, ylab=&quot;&quot;,xlab=&quot;&quot;) text(0,0,label=&quot;Not Enough Data&quot;, cex=2) }else{ Y &lt;- Xtot_sub$result #zoo::rollmean((Xtot_sub$result), 4) X &lt;- 1:length(Y) scatter.smooth(X,Y,span=20/length(Y), main = paste0(&quot;Smoothed Shooting Outcomes: Player &quot;,playernum), yaxt=&quot;n&quot;,ylab=&quot;Result&quot;,xlab=&quot;Order&quot;) axis(side=2,at=c(0,1),labels=c(&quot;Miss&quot;,&quot;Make&quot;)) } } maxseason &lt;- Xtot %&gt;% group_by(globalplayerid, season) %&gt;% summarize(num=n()) %&gt;% group_by(globalplayerid) %&gt;% mutate(m=max(num)) %&gt;% filter(num==m) %&gt;% as.data.frame() for(i in 1:nrow(playerseasons)){ r &lt;- playerseasons[i,] plotshottime(r[[1]],r[[2]],i) } We can see that the plots vary in the consistency of their made shots, since they all contain spikes and trends. For example, the third plot initially has a very high success rate, which quickly falls to the middle after about thirty shot attempts, and the second plot has a noticeable upward trend in shot success beginning around shot number one hundred fifty. We investigate the shooting outcomes using Bayesian models, and show the results in the next section. "],
["6-model.html", "Chapter 6 Models", " Chapter 6 Models For our models, we consider the shot location, the shooter identity, a home court indicator, and the shooting outcomes of nearby games as factors that can affect a shot outcome. In every model, we use Gibbs sampling in the JAGS library to build a logistic regression model that provides the posterior distribution of the shot location parameters (distance and angle). The model does not account for covariance between these predictors. In addition, we build mixed effects and discounted likelihood models to control for shooter identity and game identity, respectively. These models will show us how consistent the show location parameters are between shooters and between games. In our sampling method, we set priors using the Maximum Likelihood Estimates for the first four games in the dataset, and we initialize our chains using values of 0 for all means, and 1 for all variances. Diagnostic plots for these models can be found in Appendix number. 6.0.0.1 Generalized Linear Model In our logistic regression model, we only look at shot location and the home court indicator as predictors for the shot outcome. To look at these effects for particular players, we simply subset the dataset to shots attempted by that player before running the Markov chain. The results of the credible intervals are reported for the same four players, in the same order as they were previously presented in the Exploratory Data Analysis section. talk about probabilistic interpretations From these plots, we see that the effect of the angle contains zero, and it is therefore probably not predictive of a made shot. We also see that the 95% credible interval on the effect of distance is completely negative, which follows the intuitive idea that the probability of a made shot significantly decreases as distance from the basket increases. 6.0.0.2 Hierarchical Generalized Linear Model In this hierarchical model, we add random effects to allow the parameters to vary for each player in the dataset. We present the results below using densities of how the four players of interest compare to the team distribution, and by using contour plots that illustrate each player’s probability of a made shot by their location on the court. The hierarchical model shows us that our four high-usage players of interest do not appear to be randomly spread across the population of players. The intercept plot shows that all four are in or close to the top half of values, and the radius plot shows the opposite with the four players trending towards lower parameter values. The high intercepts make intuitive sense, because the players who take a lot of shots are generally more capable of making them under baseline conditions than their teammates are. However, the observation that these players would have a more drastic drop in field goal percentage than an average Duke player as they move farther from the basket is surprising. In the plots below, we have contour plots showing players’ expected field goal percentages at different locations on the court. Between our four players of interest, we can observe how player 1 is more effective on the ____ side of the basket than the others, and how player ____ is the ___ long-range shooter. 6.0.0.3 Discounted Likelihood Hierarchical Model In the discounted likelihood models, the likelihood of a single observation is more heavily influenced by the observations close to it than the observations far away from it. We measure the “distance” between observations by the number of games between them; a shot attempt that occurs in the next or the preceding game will influence the likelihood of the observation more than a shot that occurs two games away. insert some math notation The effect of an observed shot on the likelihood of another shot decreases as the distance between the observations increases, and as \\(\\delta\\) decreases. If larger values of \\(\\delta\\) fit the data better, this suggests that shooting success is consistent, throughout a career. If smaller values of \\(\\delta\\) are more likely in the data, however, then we can assume there is a substantial amount of time variation in the data on the game level. The values of delta that we fit the discounted likelihood model on are 0.750, 0.800, 0.850, 0.900, 0.950, and 0.999. discuss discounted likelihood and implications of time-variation for different values of delta talk about your illustrations 6.0.0.4 Evaluation of Models To evaluate these models, we use 5-fold cross validation. In each train-test split, we evaluate a model’s out-of-sample classification rate (using a cutoff probability of 0.5), Brier score (mean squared error), and log likelihood. The predictions and fitted values are obtained using MCMC averages over ___. The results are plotted below: From these plots, we can observe that all of the models have different strengths. The discounted likelihood model with the smallest \\(\\delta\\) consistently has the highest likelihood. However, it does not test as well as the other models in areas of out-of-sample classification rate and Brier score. This suggests that a smaller value of \\(\\delta\\) leads to overfitting the model to the training data, since the likelihood of a particular shot is only influenced by shots close to it. To account for possible unexplained variation between seasons, and in missing road games, I repeated this analysis on a subset of the data that only consisted of shots from the 2015 season, and a subset that was only home games. results! (show these results in Appendix?) For the remainder of this paper, we will "],
["7-disc.html", "Chapter 7 Discussion", " Chapter 7 Discussion Summarize and reflect upon results Future goals for this research are to build a better-fitting model to predict basketball shots using more advanced factors that can be approximated from the dataset; possibilities for this include using the distance of the nearest defender as a proxy for defense quality, or using the amount of time a player has played without a substitution or timeout to approximate fatigue. If you feel it necessary to include an appendix, it goes here. # The First Appendix This first appendix includes all of the R chunks of code that were hidden throughout the document (using the `include = FALSE` chunk tag) to help with readibility and/or setup. **In the main Rmd file** ```r # This chunk ensures that the thesisdowndss package is # installed and loaded. This thesisdowndss package includes # the template files for the thesis. if(!require(devtools)) install.packages(\"devtools\", repos = \"http://cran.rstudio.com\") if(!require(thesisdowndss)) devtools::install_github(\"mine-cetinkaya-rundel/thesisdowndss\") library(thesisdowndss) ``` **In Chapter \\@ref(ref-labels):** # The Second Appendix, for Fun --> "],
["A-appendix-1-code.html", "A Appendix 1: Code", " A Appendix 1: Code "],
["B-appendix-2-diagnostic-plots.html", "B Appendix 2: Diagnostic Plots B.1 GLM:", " B Appendix 2: Diagnostic Plots B.1 GLM: "],
["references.html", "References", " References "]
]
